---
title: "DSA1101 Topic 4: K-Nearest Neighbours"
author: "Dawn Cheung"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
font-style: italic;
}

</style>

## Definition of Terms
 - has n number of training points
 - features denoted as x
 - categorical response is y
 - with info x, the predicted y is Ĝ(x) [G hat (x)] or really y hat is fine too
 - since we only consider binary responses (ie 0 or 1) only, the prediction Ĝ(x) is either 0 or 1 too



### KNN

```{r,eval=TRUE, echo=FALSE}
knitr::include_graphics("~/GitHub/DSA1101 Slayers/Pictures/photo_knn small k.jpg")
```

### Scaling x

```{r,eval=TRUE, echo=TRUE}
market <- read.csv("~/GitHub/DSA1101 Slayers/datasets/Smarket.csv")
attach(market)
Lag1 = scale(Lag1) # to standardise all the predictors (the x es)
```
To be done BEFORE APPLYING KNN

Will not be needed for stock market dataset cus lap1 to lap 5 are all *similar in magnitude* => will not change the outcome a lot

### Stock Market Dataset
Predicting the direction of the stock market: whether it goes up or down => 1 is up, 0 is down

knn() will require the following arguments:

  - Matrix of predictors/features x for training
  - Matrix of predictors/features x to be predicted
  - Vector containing class labels for the training data
  - Value for k (number of nearest neighbours for the classifier)
```{r, eval=TRUE,echo=TRUE}
# Enter code here
library(class)
library(dplyr)
market <- read.csv("~/GitHub/DSA1101 Slayers/datasets/Smarket.csv")
head(market)
# we see that X is the row number,
# Lag 1 to 5, the percentage returns for the 5 prev days => these are our predictors
dim(market) #get no. rows and columns
attach(market)
```

Gameplan:

 - all years before 2005 (ie 2001 - 2004) will be training set
 - year 2005 will be testing set

```{r,eval=TRUE, echo=TRUE}
#indexes of all the rows where Year < 2005
index.train = which(Year < 2005) #returns a vector

#create data frame that has all the rows before 2005
train.data = market[index.train, ] #REMEMBER THE ,] BC MARKET IS NOT A LIST, ITS A DATAFRAME so u need specify the rows and columns
# leave blank ie [index.train,(empty)] = returns all columns

#the rest of the rows go to test data
test.data = market[-index.train, ]

dim(train.data); dim(test.data) #dim is dimentions hehe
```
Getting arguments 1 and 2
```{r,eval=TRUE, echo=TRUE}
#now we're filtering by all the columns we're gonna use, for both test and train datasets. idk why we're doing this after spiltting them, OK NOW I GETS cus we're gonna split them further into predictors and responses hehe
#REMEMBER THESE ARE DATAFRAMES SO SPECIFY ROW AND COLUMN
train.x = train.data[ ,c("Lag1", "Lag2", "Lag3", "Lag4", "Lag5")]
test.x = test.data[ ,c("Lag1", "Lag2", "Lag3", "Lag4", "Lag5")]

```
Getting argument 3
```{r,eval=TRUE, echo=TRUE}
#now get the responses to train & test the algorithm
train.y = train.data[ ,c("Direction")]
test.y = test.data[ ,c("Direction")] #these are the 'real' responses, NOTE it is NOT needed for knn()

```

### OK WE FORMING THE MODEL NOW
```{r,eval=TRUE, echo=TRUE}
library(class)

knn.pred = knn(train.x, test.x, train.y, k = 1)

knn.pred #returns the prediction for the response of the test points i.e. predictions for text.x

```

##### So how did the model do?
```{r,eval=TRUE, echo=TRUE}
data.frame(test.y, knn.pred) %>%
  slice(1:25) #lol to shorten the doc

table(test.y, knn.pred)

```

<br>

#### Diagnostics
Evaluation of the classifier's performance

More notations:

 - For 2 class labels, C is positive and C' (C prime) is negative
   - True Positive: predict C, when actually C
   - True Negative: predict C', when actually C'
   - False Positive: predict C, when actually C'
   - False Negative: predict C', when actually C
   

##### Confusion Matrix
```{r,eval=TRUE, echo=FALSE}
knitr::include_graphics("~/GitHub/DSA1101 Slayers/Pictures/Confusion Matrix Pic.jpg")
knitr::include_graphics("~/GitHub/DSA1101 Slayers/Pictures/CM spam emails.jpg")
```

##### Criteria to Evaluate

 - Accuracy
 - True Positive Rate (TPR)
 - False Positive Rate (FPR)-- Type 1 Error
 - False Negative Rate (FPR)-- Type 2 Error
 - Precision
 - ROC curve & AUC value (will learn later)
 

##### Accuracy
```{r,eval=TRUE, echo=FALSE}
knitr::include_graphics("~/GitHub/DSA1101 Slayers/Pictures/yum.jpg")
```

 - The *overall success rate*
 - Basically correct / total x 100%



##### True Positive Rate (TPR)
```{r,eval=TRUE, echo=FALSE}
knitr::include_graphics("~/GitHub/DSA1101 Slayers/Pictures/TPR.jpg")
```



##### False Positive Rate (FPR)-- Type 1 Error
```{r,eval=TRUE, echo=FALSE}
knitr::include_graphics("~/GitHub/DSA1101 Slayers/Pictures/FPR.jpg")
```




##### False Negative Rate (FNR)-- Type 2 Error
```{r,eval=TRUE, echo=FALSE}
knitr::include_graphics("~/GitHub/DSA1101 Slayers/Pictures/FNR.jpg")
```




##### Precision
```{r,eval=TRUE, echo=FALSE}
knitr::include_graphics("~/GitHub/DSA1101 Slayers/Pictures/Precision.jpg")
```

Odds ratio for 2 by 2 matrix:

[a b]

[c d]

OR = ad/bc

```{r,eval=TRUE, echo=TRUE}
slay <- function(x) {
  if (any(x == 0)){
    x = x + 0.5
  }
  return( (x[1,1]*x[2,2]) / (x[1,2]*x[2,1]) )
}

slay(cbind(c(1,2), c(3,4)))
#remember that cbind stands for column bind
#so all inputs will be put as new columns

```

k in knn is usually odd



### Accuracy

```{r,eval=TRUE, echo=TRUE}
knn.pred = knn(train.x, test.x, train.y, k = 5)
table(test.y, knn.pred)

knn.pred = knn(train.x, test.x, train.y, k = 10)
table(test.y, knn.pred)
```
<br>

## TUTORIAL QUESTIONS

1 Read the data from the file Colleges.txt. Consider a simple linear regression of percentage of applicants accepted (Acceptance) on the median combined math and verbal SAT score of students (SAT),
called Model M1.


1a) Consider data set Colleges.txt. **Write a function in R using the matrix approach to perform a
simple linear regression of percentage of applicants accepted (Acceptance) on the median combined
math and verbal SAT score of students (SAT).**

```{r, eval=TRUE,echo=TRUE}
library(tidyverse)
library(dplyr)
collegesdb <- read.csv("~/Github/DSA1101 Slayers/datasets/Colleges.txt", sep = "\t", header = TRUE)
attach(collegesdb)


matrix <- function(x, y){
  #rem x and y are matrixes
  #aim: get beta hat (by minimising RSS, etc)
  beta <- solve(t(x)%*%x)%*%t(x)%*%y
  return(beta)
  #solve() gets the inverse of the function
  #t() is transpose which is Aᵀ the T thing
  #%*% multiplies the matrixes, like multiplication sign for matrixes
}
matrix(x = cbind(1, SAT), y = Acceptance)
#bcos x is a 2 by n matrix, and y is a 1 by n

```
Compare the results with the answers in part (b) of Question 1.

```{r, eval=TRUE,echo=TRUE}
#simple(SAT, Acceptance)
#simple() is NOT A BUILT IN FUNCTION is from tut 3

lm(Acceptance ~ SAT)

```




1b) If data set of n points has two input features, x1, x2, by matrix approach, the estimate of coefficient is still ̂ β = (XT X)−1XT y

i. Specify **matrix y, X and β.**
ii. Use your function in part (a) to **perform a multivariate linear regression of percentage of
applicants accepted (Acceptance) on SAT and Top.10p - percentage of students in the top 10% of their high school graduating class**


for i,

   y = response variable, Acceptance
   
   X = matrix of columns 1, SAT and Top.10p
   
   β = da unknowns 
   
```{r, eval=TRUE,echo=TRUE}
# Define matrix X
X = cbind(1, SAT, Top.10p)
matrix(x = X, y = Acceptance)

```

Compare the results with using lm()

```{r, eval=TRUE,echo=TRUE}
lm(Acceptance ~ SAT + Top.10p)


```
<br>

2. A dataset on house selling price was randomly collected 1, house_selling_prices_FL.csv. It’s our
interest to model how y = selling price (dollar) is dependent on x = the size of the house (square feet).
A simple linear regression model (y regress on x) was fitted, called Model 1.
The given data has another variable, NW, which specifies if a house is in the part of the town considered less desirable (NW = 0).

**2a) Derive the correlation between x and y.**


```{r, eval=TRUE,echo=TRUE}
#enter code here
library(dplyr)
library(class)
library(tidyverse)
housey = read.csv("~/GitHub/DSA1101 Slayers/datasets/house_selling_prices_FL.csv")
attach(housey)
NW = as.factor(NW) #declaring NW as categorical, just to clean the data before starting


cor(price, size)
cor(size, price)

```


**2b)Derive a scatter plot of y against x. Give your comments on the association of y and x. **


```{r, eval=TRUE,echo=TRUE}
# Enter code here
M1 = lm(price ~ size)

plot(size, price, pch = 20)
```
 - Positive relationship

 - Linear correlation: no curving of points

 - The variability of response is quite stable: range of y does not increase as x increases


2c) Derive R2 of Model 1. Verify that √R2 = |cor(y,x)|. In which situation we can have √R2 = cor(y, x)

```{r, eval=TRUE,echo=TRUE}
M1 = lm(price ~ size)
summary(M1) #R^2 is hidden somewhere here

summary(M1)$r.squared #SPELL SQUARED CORRECTLY PLS 

(cor(price, size))^2 # its cor to corr hor

#they will be very similar when:
#if cor(y, x) > 0, and M1 is simple y ~ x (simple model)
#why: prove cor(y, yhat) = cor(y, x) 
# prove cor^2(y, yhat) = var(yhat)/var(y)
# claim var(yhat)/var(y) = R^2

#where the explainatory is quantitative
```

2d) Form a model (called Model 2) which has two regressors (x and NW). Write down the equation
of Model 2.

```{r 2d, eval=TRUE,echo=TRUE}
# Enter code here
NW = as.factor(NW) #declaring NW as categorical

M2 = lm(price ~ size + NW)
summary(M2)
#notice that since NM is catagorical, it starts calling the variable NW1 to show that it is an indicator, and it is indicating that NW = 1 == True => I(NW = 1)is the indicator lol
M2$coeff
paste0("the equation is: price(hat) = ", M2$coeff[1], " + ", M2$coeff[2], "size + ",M2$coeff[3], "I(NW = 0)")
```



2e) Report the coefficient of variable NW in Model 2. Interpret it.

```{r 2e, eval=TRUE,echo=TRUE}
# Enter code here
M2$coeff[3]
```

2 houses of the same location, then one with size increase by 1 unit will have average price larger by $77.98 [must be same location]

for houses of the same size, the one not at the NW area (or NW = 1) will be more expensive by $3059 on average

<br>

2f) Estimate the price of a house where its size is 4000 square feet and is located at the more desirable
part of the town

```{r 2f, eval=TRUE,echo=TRUE}
# Enter code here

new_data = data.frame(size = 4000, NW = "1") #creates 2 columns
#also NW = "1" must be in quotes cus its a factor vairable (not a number)

predict(M2, newdata = new_data)

```

2g) Report the R2 of Model 2. Interpret it

```{r 2g, eval=TRUE,echo=TRUE}
# Enter code here
summary(M2)$r.squared
```
Relatively low (quite far from 1) => low amount of variability inherent in the response before the regression is performed => M2 BETTER than model 1, which has a higher R^2 (so M1 has higher variability than M2) 

<br>

Extension question: is M2 significant?

Significance => look at F statistic's p-value

since p-value < 2.2e-14
compared to a model with no regressors, just an intercept (straight line), M2 does significantly better



<br>
<br>





