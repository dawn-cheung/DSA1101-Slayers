---
title: 'DSA1101 Topic 5: Decision Trees'
author: "Dawn Cheung"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




## Bank Sample Dataset

```{r pressure, echo=TRUE}
library(tidyverse)
library(class)
library(rpart)
library(rpart.plot) #remember its a diff package as rpart!!
bankdata = read.csv("~/Github/DSA1101 Slayers/datasets/bank-sample.csv")
attach(bankdata)

#response variable: subscribed (yes/no)
#we have 16 features => not using all (we will use 8)

fit <- rpart(subscribed ~ job + marital + education + default + housing + loan + contact + poutcome,
             method = "class", #tells R to return the category of response (yes or no)
             data = bankdata,
             control = rpart.control(minsplit = 1), #tells R how to split the node; minsplit = 1 means a branch is created when there is at least 1 observation in that branch => tells R how big we want the tree to be
             parms = list(split = "information")) # what criteria to use to choose the root node & internal nodes; either info gained or gini index; default val is gini

summary(fit)

#to fit the plotted tree:
rpart.plot(fit, type = 4, extra = 2, clip.right.labs = FALSE)
```

Ok notice the first node shown is a modal category of the response ie it starts with a no (which is the majority)

So not the root node (which needs to be an input vairable)

# Choosing the root node
## Why was poutcome selected as the decsion variable at the root node?
i.e. why was poutcome chosen by the algorithm  to be the first split? finding the most useful feature in the dataset to add to the tree


Selecting the most informative attribute based on 2 basic measures:

 - Entropy: the impurity of an attribute
 - Information gain: the reduction in purity should a split be made there

## Purity
Its probability of the corresponding class

 - When only considering the particular response variable, ignoring all other attributes/features, then the probability of [the response variable being equal to something] is the purity
  - eg it is 89.45% pure on the class where [the response variable is equal to something], and the rest, 10.55% pure on the class [variable equals to smth else]


## Entropy
```{r,eval=TRUE, echo=FALSE}
knitr::include_graphics("~/GitHub/DSA1101 Slayers/Pictures/entropy formula.jpg")
```

```{r,eval=TRUE, echo=TRUE}
p = seq(0, 1, 0.1)
Entropy = -(p*log2(p) + (1-p)*log2(1-p))
plot(p, Entropy, ylab = "Entropy", xlab = "P(y = 1)")
lines(p, Entropy, col = "red")
```

```{r,eval=TRUE, echo=TRUE}
#how the catagories are split:
# x1 = failure, other, unknown
# x2 = success

x1 = which(bankdata$poutcome != "success")
#index of rows which poutcome = x1

length(x1)
#we find that 1942 rows have poutcome = x1

#probability of poutcome != success
length(x1) / 2000 #200 is the total number of entires

x2 = which(bankdata$poutcome == "success")

length(x2)
#we find that 58 rows have poutcome = x2 = success

length(x2) / 2000 #200 is the total number of entires


#FREQUENCY TABLE AH DONT FORGET
table(bankdata$subscribed[x1])
#among 1942 customers with poutcome = x1, 179 subscribed (179 yes), and 176

table(bankdata$subscribed[x2])
#among 58 customers with poutcome = x2, 32 subscribed (32 yes), and 26

```


## Traversing down the tree, how are the subsequent decision variables at each node selected?
i.e. understanding how the algorithm decides what should be a branch and what should not be made into a branch

### Playing with maxdepth, minsplit, cp (complexity parameter) in control = rpart.control()

```{r nopressure, echo=TRUE}

#maxdepth = 4:
fit <- rpart(subscribed ~ job + marital + education + default + housing + loan + contact + poutcome,
             method = "class", #tells R to return the category of response (yes or no)
             data = bankdata,
             control = rpart.control(maxdepth = 4), #tells R how to split the node; minsplit = 1 means a branch is created when there is at least 1 observation in that branch => tells R how big we want the tree to be
             parms = list(split = "information")) 

rpart.plot(fit, type = 4, extra = 2, clip.right.labs = FALSE)


#maxdepth = 1:
fit <- rpart(subscribed ~ job + marital + education + default + housing + loan + contact + poutcome,
             method = "class", #tells R to return the category of response (yes or no)
             data = bankdata,
             control = rpart.control(maxdepth = 1), #tells R how to split the node; minsplit = 1 means a branch is created when there is at least 1 observation in that branch => tells R how big we want the tree to be
             parms = list(split = "information")) 

rpart.plot(fit, type = 4, extra = 2, clip.right.labs = FALSE)

#minsplit = 15:
fit <- rpart(subscribed ~ job + marital + education + default + housing + loan + contact + poutcome,
             method = "class", #tells R to return the category of response (yes or no)
             data = bankdata,
             control = rpart.control(minsplit = 15), #tells R how to split the node; minsplit = 1 means a branch is created when there is at least 1 observation in that branch => tells R how big we want the tree to be
             parms = list(split = "information")) # what criteria to use to choose the root node & internal nodes; either info gained or gini index; default val is gini

#to fit the plotted tree:
rpart.plot(fit, type = 4, extra = 2, clip.right.labs = FALSE)

#cp = 0.001
fit <- rpart(subscribed ~ job + marital + education + default + housing + loan + contact + poutcome,
             method = "class", #tells R to return the category of response (yes or no)
             data = bankdata,
             control = rpart.control(cp = 0.001), #tells R how to split the node; minsplit = 1 means a branch is created when there is at least 1 observation in that branch => tells R how big we want the tree to be
             parms = list(split = "information")) # what criteria to use to choose the root node & internal nodes; either info gained or gini index; default val is gini
rpart.plot(fit, type = 4, extra = 2, clip.right.labs = FALSE)
```

As maxdepth increases, tree becomes larger.

As minsplit increases, tree becomes smaller.

As cp decreases, tree becomes larger. [default is cp = 0.01]


### How to test which parameters are best? Just use for loops to try each parameter and attain accuracy lol


### Playing with varlen, faclen, clip.right.labs, type in rpart.plot()
- varlen = variable length
  - 0: full name will be written
- faclen = factor length
  - 0: full name will be written
- clip.right.labs literally means the name of the variable wont be repeated on the right branch
  - TRUE or FALSE
  
MEANING OF THE TREE WILL REMAIN THE SAME
```{r kejkjpressure, echo=TRUE}


fit <- rpart(subscribed ~ job + marital + education + default + housing + loan + contact + poutcome,
             method = "class", #tells R to return the category of response (yes or no)
             data = bankdata,
             control = rpart.control(maxdepth = 4), #tells R how to split the node; minsplit = 1 means a branch is created when there is at least 1 observation in that branch => tells R how big we want the tree to be
             parms = list(split = "information")) 

#type = 2:
rpart.plot(fit, type = 2, extra = 2, clip.right.labs = FALSE)
```

### Analysis of Fitted Tree
modal catagory => the major/majority catagory
```{r kekpressure, echo=TRUE}
fit <- rpart(subscribed ~ job + marital + education + default + housing + loan + contact + poutcome,
             method = "class", #tells R to return the category of response (yes or no)
             data = bankdata,
             control = rpart.control(maxdepth = 4), #tells R how to split the node; minsplit = 1 means a branch is created when there is at least 1 observation in that branch => tells R how big we want the tree to be
             parms = list(split = "information")) 

#type = 2:
rpart.plot(fit, type = 2, extra = 2, clip.right.labs = FALSE)
```

how model decides how to structure the tree: by splitting at this depth, it reduces the entropy the most


## DTdata.csv [play_decsion]
```{r DT data, echo=TRUE}
library(rpart)
library(rpart.plot) #remember its a diff package as rpart!!
play_decsion = read.csv("~/Github/DSA1101 Slayers/datasets/DTdata.csv")
attach(play_decsion)

newdata = data.frame(Outlook = "rainy", Temperature = "mild", Humidity = "")

```

how model decides how to structure the tree: by splitting at this depth, it reduces the entropy the most



## TUTORIAL QUESTIONS

(MLR) Consider the horseshoe female crab data given in the csv file crab.csv. We would want to
form a model for the weight of the female crabs (kg), which depends on its width (cm) and its spine
condition (1 = both good, 2 = one worn or broken, 3 = both worn or broken).

(a) Produce a scatter plot of variable weight against width for different condition of spine.
```{r 1a, echo=TRUE}
library(tidyverse)

crabdata = read.csv("~/Github/DSA1101 Slayers/datasets/crab.csv")

#REMEMBER that when converting to factor/catagorical, address it as part of he crabdata dataset, so dataset$variable
crabdata$spine  = as.factor(crabdata$spine) 
attach(crabdata)
glimpse(crabdata)


plot(weight, width, pch = 20)

```

(b) Fit a linear regression model for weight which has two explanatories, width and spine.
```{r 1b, echo=TRUE}
M1 = lm(weight ~ width + spine, crabdata)
summary(M1)
```

(c) Is the fitted model significant?
    Yes

When looking at the F-statistic's p-value, it is extremely small, p < 2.2e-16. Hence, compared to a model with no regressors (such that it is a straight horizontal line), the model in (b) does significantly better


(d) Derive R2 and adjusted R2 of the fitted model.
```{r 1d, echo=TRUE}
paste0("R2 is ", summary(M1)$r.squared)

paste0("Adjusted R2 is ", summary(M1)$adj.r.squared)
```


(e) Write down the fitted model
```{r 1e, echo=TRUE}
paste0("y hat  = ", M1$coeff[0], "+ ", M1$coeff[1], "width + ", M1$coeff[2]," x I(spine  = 2) + ", M1$coeff[3], " x I(spine  = 3)")
```

(f) Two female crabs of the same width, find the difference of their weight if one has spines are of
good condition and another one with broken spines
```{r 1f, echo=TRUE}
0.05544 #?? not sure
#WRONG go check the ans again

```

(g) Predict the weight of a female crab that has width of 27 cm and has both spines worn or broken.
```{r 1g, echo=TRUE}
q = data.frame(width = c(27), spine = c("3"))

predict(M1, newdata = q)


```



The K-nearest neighbor classfier

The table below provides a training data set containing six observations, three predictors, and one
qualitative response variable, Y .
    
    Obs X1 X2 X3 Y
    1   0  3  0  Red
    2   2  0  0  Red
    3   0  1  3  Red
    4   0  1  2  Green
    5  -1  0  1  Green
    6   1  1  1  Red
    
Suppose we wish to use this data set to make a prediction for Y when X1 = X2 = X3 = 0 using
K-nearest neighbors.

(a) Compute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0.
```{r 2a, echo=TRUE}

Q = cbind(X1 = c(0, 2, 0, 0 ,-1, 1),
      X2 = c(3, 0, 1, 1 , 0, 1),
      X3 = c(0, 0, 3, 2 , 1, 1))

for (i in 1:6) {
  Ed = sqrt( (Q[[i, 1]]-0)^2 + (Q[[i, 2]]-0)^2 + (Q[[i, 3]]-0)^2 )
  print(Ed)
}

```

(b) What is our prediction with K = 1? Why?
```{r 2b, echo=TRUE}
library(class)
library(dplyr)

#train.x is Q

test.x = cbind(X1 = c(0),
      X2 = c(0),
      X3 = c(0))

train.y = c("Red", "Red", "Red", "Green", "Green", "Red")

knn.pred = knn(Q, test.x, train.y, k = 1)

knn.pred
#predict not needed here

#oops u were supposed to use eucledian distances only and determine manually

```

(c) What is our prediction with K = 3? Why?
```{r 2c, echo=TRUE}
knn.pred = knn(Q, test.x, train.y, k = 3)

knn.pred
#When k = 3, the 3 nearest points are the 5th (Green), 6th (Red), and 2nd (Red).Hence , the test point will be classifed as Red
```

(d) If the Bayes decision boundary (the gold standard decision boundary) in this problem is highly non-linear, then would we expect the best value for K to be large or small? Why?

Small k. Non linear means boundary line would be very flexible since it is influenced by local features of a handful of traning data points.

Small k translates to a more flexible classification model



<br>
Measures of classifier performance

Suppose we have developed a K-nearest neighbors classifier for predicting diabetes status. The following table shows the actual response Y (1 =yes, 0 =no) and fitted value Yb using the classifier for 10
test data points. A test data point is predicted to be Gb = 1 if Y > δ ˆ , for a specified threshold value δ.
(Recall that we use δ = 0.5 in class, also known as the majority rule).

We define

  TPR = TP / TP + FN ; FPR = FP / FP + TN.
  
For each of the thresholds δ = 0.3, 0.6 and 0.8, *derive TPR and FPR* in making predictions with the K-nearest neighbors classifier for the 10 test data points. *Plot TPR against FPR for the three thresholds.*

```{r 3a i am in pain, eval = FALSE, echo=TRUE}
Yi = c(1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 0L)
Yi_hat = c(0.9, 0.5, 0.7, 0.4, 0.5, 0.2, 0.7, 0.9, 0.1, 0.1)
Why = cbind(Yi, Yi_hat)

#Yi[which(Yi == 0L)] = "help"

Why = ifelse(which(Yi == 1L) == which(Yi_hat > 0.3), "TP", Why)
Why = ifelse(which(Yi == 0L) == which(Yi_hat < 0.3), "TN", Why)
Why = ifelse(which(Yi_hat < 0.3), "FN", Why)
Why = ifelse(which(Yi_hat < 0.3), "FP", Why)

Why

#Why[which(Why$Yi == 1L && Why$Yi_hat > 0.3)] = "TP"

```





```{r 3a failed working chunk, eval = FALSE, echo=TRUE}
Yi = c(1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 0L)
Yi_hat = c(0.9, 0.5, 0.7, 0.4, 0.5, 0.2, 0.7, 0.9, 0.1, 0.1)

TPRyeet = c()
FPRyeet = c()

gginsane <- function(d) {
  TP = 0
  FP = 0
  TN = 0
  FN = 0
  for (i in 1:length(Yi)) {
    if (Yi_hat[i] > d) {
      if (Yi[i] == 1) {
        TP = TP + 1
      }
      if (Yi[i] == 0L) {
        FN = FN + 1
      }
    } else if (Yi_hat[i] < d) {
      if (Yi[i] == 0L) {
        TN = TN + 1
        }
      if (Yi[i] == 1) {
        FP = FP + 1
        }
    }
  }
  TPR = TP/(TP + FN)
  FPR = FP/(FP + TN)
  return(c(TPR, FPR))
}

gginsane(0.3)
gginsane(0.6)
gginsane(0.8)

TPRyeet = c(gginsane(0.3)[1], gginsane(0.6)[1], gginsane(0.8)[1])
FPRyeet = c(gginsane(0.3)[2], gginsane(0.6)[2], gginsane(0.8)[2])

plot(FPRyeet, TPRyeet)


```
Error in if (Yi[i] == 0L) { : missing value where TRUE/FALSE needed WHAT IN THE POTATO

update: re-ran the code the next day without edits. works perfectly. WHY PLS OMG I AM ON THE VERGE




 i  Yi Yˆi
 1  1  0.9
 2  1  0.5
 3  0  0.7
 4  1  0.4
 5  1  0.5
 6  0  0.2
 7  0  0.7
 8  1  0.9
 9  0  0.1
 10 0  0.1

b) Can we add the two points (0, 0) and (1, 1) to the plot of TPR against FPR in part (a). Explain
why or why not

```{r 3b, echo=TRUE}
#hello
```
1) check if TPR and FPR can be equal to 0 or 1
2) check if TPR and FPR can be 0 or 1 at the same time

make G_hat to be always 0
=> make threshold delta to be extremely large, strictly larger than fitted value eg delta > 0.9, all test data points are predicted as 0

same logic for TPR = FPR = 1, make threshold so low that every test datapoint is predicted as 1




4. The CSV file Caravan.csv contains data on 5822 real customer records on caravan insurance purchase.
This data set is owned and supplied by the Dutch data mining company, Sentient Machine Research, and is based on real world business data. Each record consists of 86 variables, containing sociodemographic data (variables 1-43) and product ownership (variables 44-86). Variable 86 (Purchase) indicates whether the customer purchased a caravan insurance policy.

For this business, assume that the overall error rate (equivalently, the accuracy) is not of interest.
Instead, the company wants to use the classifier to predict who are the potential customers likely to purchase insurance. Then the metric precision will be important, since it relates the proportion of individuals who will actually purchase the insurance, among the group of individuals who are predicted to purchase insurance