sur = ifelse(Survived == "Yes", 1,0)
sur = as.factor()
library(dplyr)
library(ROCR)
ncol(data)
sur = ifelse(Survived == "Yes", 1,0)
data
# > typeof(data.frame(data[,-ncol(data)]))
# [1] "list"
#HUHHHH
#do NB first
nb_prediction = predict(M1,
newdata = data.frame(data[,-ncol(data)]),
type = "raw")
dt_prediction = predict(M2, newdata = data.frame(data[,-ncol(data)]), type = "raw")
library(dplyr)
library(ROCR)
ncol(data)
sur = ifelse(Survived == "Yes", 1,0)
data
# > typeof(data.frame(data[,-ncol(data)]))
# [1] "list"
#HUHHHH
#do NB first
nb_prediction = predict(M1,
newdata = data.frame(data[,-ncol(data)]),
type = "raw")
dt_prediction = predict(M2, newdata = data.frame(data[,-ncol(data)]), type = "prob")
nb_predicted_score = nb_prediction[,c("Yes")]
dt_predicted_score = dt_prediction[,c("Yes")]
actual_class = data$Survived == "Yes"
nb_pred = prediction(nb_predicted_score, actual_class)
dt_pred = prediction(dt_predicted_score, actual_class)
nb_perf <- performance(nb_pred , "tpr", "fpr")
dt_perf <- performance(dt_pred , "tpr", "fpr")
auc1 <- performance(pre_nb, "auc")@y.values[[1]]
library(dplyr)
library(ROCR)
ncol(data)
sur = ifelse(Survived == "Yes", 1,0)
data
# > typeof(data.frame(data[,-ncol(data)]))
# [1] "list"
#HUHHHH
#do NB first
nb_prediction = predict(M1,
newdata = data.frame(data[,-ncol(data)]),
type = "raw")
dt_prediction = predict(M2, newdata = data.frame(data[,-ncol(data)]), type = "prob")
nb_predicted_score = nb_prediction[,c("Yes")]
dt_predicted_score = dt_prediction[,c("Yes")]
actual_class = data$Survived == "Yes"
nb_pred = prediction(nb_predicted_score, actual_class)
dt_pred = prediction(dt_predicted_score, actual_class)
nb_perf <- performance(nb_pred , "tpr", "fpr")
dt_perf <- performance(dt_pred , "tpr", "fpr")
auc1 <- performance(nb_pred, "auc")@y.values[[1]]
#REMEMBER MUST CONVERT TO 0 and 1
sur = (Survived == "Yes")
sur = as.factor(sur)
data$sur = sur #so sur is a completely new column to mutate onto the database
attach(data)
glimpse(data)
#note that glm is an in-built function-- no library needed
M3 <- glm(sur ~ Age + Sex + Class,
data = data,
family = binomial(link = "logit")) #family decides the transformation g from g(y)
summary(M3)
library(ROCR)
#ROC for Log Reg
pred = predict(M3, type = "response")
preObj = prediction(pred, data$Survived) #notice here we don't need to use sur, the OG column is fine
rocObj = performance(preObj, measure = "tpr", x.measure = "fpr")
plot(rocObj)
#getting AUC value for Log Reg
aucLR = performance(preObj, measure = "auc")
aucLR@y.values[[1]] #0.7597259
#ROC for NB
#Naive Bayes requires more formatting!!!
naiveB = predict(M1, data[1:3], type = "raw") #for NB u need to specify what response variables (from the database) are needed
score = naiveB[, 2]
#OK SO naiveB[,c("Yes")] and naiveB[,2] IS THE SAME bc u see what predict() returns
#                No       Yes
#   [1,] 0.69605930 0.3039407
#   [2,] 0.69605930 0.3039407
#   [3,] 0.69605930 0.3039407
#yea so basically u want that Yes column
#these are the predicted Yes'es by the way
preObjNB = prediction(score, data$sur)
rocObjNB = performance(preObjNB, measure = "tpr", x.measure = "fpr")
plot(rocObjNB, add = TRUE, col = "red") #so to add on to our prev graph
#getting AUC value for NB
aucNB = performance(preObjNB, measure = "auc")
aucNB@y.values[[1]]
legend("bottomright", c("Logistic Regression", "Naive Bayes"), col = c("black", "red"), lty = 1)
preObjNB
rocObjNB
score <- predict(M3, data[,2:5], "raw")[,2]
M4 <- rpart(Status ~ .,
method = "class",
data = data,
control = rpart.control(cp = 0.001),
parms = list(split = "gini"))
source("~/GitHub/DSA1101 Slayers/Finals Working Materials/past year paper working.R", echo=TRUE)
df1 = read.csv("~/Github/DSA1101 Slayers/datasets/crab.csv")
glimpse(df1)
source("~/GitHub/DSA1101 Slayers/Finals Working Materials/A0286548L.R", echo=TRUE)
#Q2
hist(satell)
df1 = read.csv("~/Github/DSA1101 Slayers/datasets/crab.csv")
glimpse(df1)
df1 = read.csv("~/Github/DSA1101 Slayers/datasets/crab.csv")
glimpse(df1)
attach(df1)
#Q2
hist(satell)
#Q2
plot(satell, width)
df1$color = as.factor(df1$color)
df1$spine = as.factor(df1$spine)
attach(df1)
M1 = lm(satell ~ color + spine + width + weight,
data = df1)
#Q4
summary(M1)$r.squared
#Q4
summary(M1)
#Q4
summary(M1)
#Q5
df1$status = satell[which(satell > 0)]
satell[which(satell > 0)]
#Q5
df1$status = c()
df$status = ifelse(satell[which(satell > 0)], 1, 0)
status = ifelse(satell[which(satell > 0)], 1, 0)
df1$status = status
#Q5
df1$status = numeric(length(satell))
status = ifelse(satell[which(satell > 0)], 1, 0)
df1$status = status
#Q5
df1$status = numeric(173)
status = ifelse(satell[which(satell > 0)], 1, 0)
df1$status = status
## PART 2
satell
## PART 2
length(satell)
status
length(satell)
#Q5
df1$status = numeric(173)
status = ifelse(satell[which(satell > 0)], 1L, 0L)
df1$status = status
df1$status = numeric(173)
df1$status
for (i in 1:length(df1$satell)) {
if (satell[i] == 0) {
df1$status[i] = 0
} else {
df1$status[i] = 0
}
}
df1$status
df1$status
for (i in 1:length(df1$satell)) {
if (satell[i] == 0) {
df1$status[i] = 0
} else {
df1$status[i] = 1
}
}
df1$status
attach(df1)
length(which(df1$status == 1))
freqtable = table(color, status)
View(df1)
View(df1)
freqtable = table(status, color)
freqtable = table(status, color)
length(status)
length(color)
length(which(df1$status == 1))
length(which(df1$status == 0))
df1$status = as.factor(df1$status)
attach(df1)
length(status)
length(color)
length(df1$status)
length(color)
source("~/GitHub/DSA1101 Slayers/Finals Working Materials/A0286548L.R", echo=TRUE)
length(which(df1$status == 1))
freqtable = table(df1$status, df1$color)
CS.table = prop.table(freqtable)
CS.table
#Q7
freqtable = table(df1$color, df1$status,)
#Q7
freqtable = table(df1$color, df1$status)
CS.table = prop.table(freqtable)
CS.table #print contingency table
freqtable
CS.table[1,1] / CS.table[1,2]
CS.table[2,1] / CS.table[2,2]
CS.table[3,1] / CS.table[3,2]
CS.table[4,1] / CS.table[4,2]
paste0("log[phat/(1-phat)] = ",
M2$coeff[1]," + ", M2$coeff[2],
"Life_expectancy + ", M2$coeff[3],
"Adult_mortality + ", M2$coeff[4],
"infant_deaths + ", M2$coeff[5], "Alcohol")
#Q9
M2 <- glm(status ~ width + weight + color,
data = df1,
family = binomial(link ="logit"))
View(aucLM)
library(tidyverse)
library(dplyr) # Piping (%>%)
library(class) #K-nearest neighbours
library(rpart) #Decision Tree
library(rpart.plot) #Decision Tree
library(e1071) #Naive Bayes
library(ROCR) #ROC / AUC
library(arules) #Association Rules
library(arulesViz) #Association Rules
set.seed(666)
#Q1a
# False
#Q1b
# True
#Q1c
# False
#Q1d
# True
#Q1e
# True
#Q2a
# B
#Q2b
# C
#Q2c
# A
#Q2d
# C
#Q2e recheck
# D
# QUESTION 3
# PART 1
#Q1
df1 = read.csv("~/Github/DSA1101 Slayers/datasets/crab.csv")
glimpse(df1)
attach(df1)
hist(satell)
# the distribution is very right skewed
# the median is around 0, where the frequency is substancially higher than all other satell values, at above 80
#Q2
plot(satell, width)
# there is no linear correlation, as the width does not seem to impact the satell value
# majority of the points are within 7-5 for satell and 22-10 for width
#Q3
df1$color = as.factor(df1$color)
df1$spine = as.factor(df1$spine)
attach(df1)
M1 = lm(satell ~ color + spine + width + weight,
data = df1)
summary(M1)$r.squared
# the R^2 value is 0.151
# this is very low
#Q4
summary(M1)
# the most insignificant feature is I(color = 4) because it has the lowest Pr(>|t|) value.
# therefore the color is the most insignificant
## PART 2
length(satell)
#Q5
df1$status = numeric(173)
status = ifelse(satell[which(satell > 0)], 1L, 0L)
df1$status
for (i in 1:length(df1$satell)) {
if (satell[i] == 0) {
df1$status[i] = 0
} else {
df1$status[i] = 1
}
}
df1$status
attach(df1)
#Q6
length(which(df1$status == 1))
length(which(df1$status == 0))
df1$status = as.factor(df1$status)
attach(df1)
#number of crabs with at least one satellite is 111
# answer: 111
#Q7
freqtable = table(df1$color, df1$status)
CS.table = prop.table(freqtable)
CS.table #print contingency table
freqtable
#number of female crabs that are of medium color and has at least a satellite is 69
#ans: 69
#Q8
CS.table[1,1] / CS.table[1,2]
CS.table[2,1] / CS.table[2,2]
CS.table[3,1] / CS.table[3,2]
CS.table[4,1] / CS.table[4,2]
# light color: 0.333
# medium color: 0.3767
# dark color:  0.692
# darker color: 2.14
#as the color becomes darker, the conditional proportion increases,
# where the highest conditional proportion is for the dakerst color at 2.14
#Q9
M2 <- glm(status ~ width + weight + color,
data = df1,
family = binomial(link ="logit"))
summary(M2)
paste0("log[phat/(1-phat)] = ",
M2$coeff[1]," + ", M2$coeff[2],
"width + ", M2$coeff[3],
"weight + ", M2$coeff[4],
" * I(color == 3) + ", M2$coeff[5],
" * I(color == 4) + ", M2$coeff[6],
" * I(color == 4) + ", M2$coeff[7])
#
paste0("log[phat/(1-phat)] = ",
M2$coeff[1]," + ", M2$coeff[2],
"width + ", M2$coeff[3],
"weight + ", M2$coeff[4],
" * I(color == 3) + ", M2$coeff[5],
" * I(color == 4) + ", M2$coeff[6],
" * I(color == 4)")
#Q9
M2 <- glm(status ~ width + weight + color,
data = df1,
family = binomial(link ="logit"))
Aframe = data.frame(width = 26, weight = 2.6, color = 4, spine = 1)
predict(M2, Aframe, type = "prob")
#Crab A:
Aframe = data.frame(width = 26, weight = 2.6, color = 4, spine = 1)
predict(M2, Aframe, type = "response")
#Crab A:
Aframe = data.frame(width = 26, weight = 2.6, color = "4", spine = 1)
predict(M2, Aframe, type = "response")
# B
Bframe = data.frame(width = 30, weight = 4.0, color = "2", spine = 3)
predict(M2, Bframe, type = "response")
score = predict(M2, df1$status, type = "response")
score
score = predict(M2, df1[,1:5], type = "response")
pred <- prediction(score , df[,6])
score = predict(M2, df1[,1:5], type = "response")
pred <- prediction(score , df1[,6])
perf <- performance(pred , "tpr", "fpr")
plot(perf, lwd = 2)
abline(a = 0,
b = 1,
col = "blue",
lty = -3)
plot(perf, lwd = 2)
abline(a = 0, b = 1, col = "blue", lty = -3)
auc = performance(pred, measure = "auc")
auc@y.values[[1]]
alpha <- round(as.numeric(unlist(perf@alpha.values)) ,4)
length(alpha)
score = predict(M2, df1[,1:5], type = "response")
pred <- prediction(score , df1[,6])
perf <- performance(pred , "tpr", "fpr")
plot(perf, lwd = 2)
#plot complete
auc = performance(pred, measure = "auc")
auc@y.values[[1]]
#auc value: 0.7775356
alpha <- round(as.numeric(unlist(perf@alpha.values)) ,4)
length(alpha) #its 164 very low pls dont fault me
fpr <- round(as.numeric(unlist(perf@x.values)) ,4)
tpr <- round(as.numeric(unlist(perf@y.values)) ,4)
plot(alpha, tpr ,xlab ="Threshold", xlim =c(0 ,1) ,
ylab = "True positive rate ", type ="l", col = "blue")
par(new ="True")
plot(alpha, fpr, xlab ="", ylab ="", axes=F, xlim =c(0, 1) , type ="l", col = "red" )
axis(side =4) # to create an axis at the 4th side
mtext(side =4, line =3, "False positive rate")
text(0.18 ,0.18 , "FPR")
text(0.58 ,0.58 , "TPR")
alpha <- round(as.numeric(unlist(perf@alpha.values)) ,4)
length(alpha) #its 164 very low pls dont fault me
fpr <- round(as.numeric(unlist(perf@x.values)) ,4)
tpr <- round(as.numeric(unlist(perf@y.values)) ,4)
plot(alpha, tpr ,xlab ="Threshold", xlim =c(0 ,1) ,
ylab = "True positive rate ", type ="l", col = "blue")
par(new ="True")
plot(alpha, fpr, xlab ="", ylab ="", axes=F, xlim =c(0, 1) , type ="l", col = "red" )
axis(side =4) # to create an axis at the 4th side
mtext(side =4, line =3, "False positive rate")
text(0.5 ,0.18 , "FPR")
text(0.88 ,0.61 , "TPR")
cbind(alpha, tpr, fpr)[70:150,]
perf <- performance(pred , "accuracy")
M3 = naiveBayes(status ~ color + spine + width + weight, df1)
results <- predict(df1$status,df1[1:5],"class"); results
M3 = naiveBayes(status ~ color + spine + width + weight, df1)
results <- predict(df1$status,df1[1:5],"class")
M3 = naiveBayes(status ~ color + spine + width + weight, df1)
results <- predict(status,df1[1:5],"class")
M3 = naiveBayes(status ~ color + spine + width + weight, df1)
results <- predict(M3, df1[1:5],"class")
results
confusion.matrix = table(status, results)
M3 = naiveBayes(status ~ color + spine + width + weight, df1)
results <- predict(M3, df1[1:5],"class")
results
confusion.matrix = table(df1$status, results)
confusion.matrix
diag(confusion.matrix)
accuracy = sum(diag(confusion.matrix))/sum(confusion.matrix)
accuracy
predict(M3, Aframe, "class")
predict(M3, Bframe, "class")
#18
predict(M3, Aframe, "prob")
predict(M3, Aframe, "raw")
predict(M3, Bframe, "raw")
M3 = naiveBayes(status ~ color + spine + width + weight, df1)
results <- predict(M3, df1[1:5],"class")[,2]
M3 = naiveBayes(status ~ color + spine + width + weight, df1)
results <- predict(M3, df1[1:5],"class")
results
confusion.matrix = table(df1$status, results)
confusion.matrix
diag(confusion.matrix)
accuracy = sum(diag(confusion.matrix))/sum(confusion.matrix)
accuracy
#report accuracy: 0.7052023
#18
predict(M3, Aframe, "raw")[,2]
predict(M3, Bframe, "raw")[,2]
#Crab A:
Aframe = data.frame(width = 26, weight = 2.6, color = "4", spine = "1")
predict(M2, Aframe, type = "response")
#probability for A: 0.681139
# B
Bframe = data.frame(width = 30, weight = 4.0, color = "2", spine = "3")
predict(M2, Bframe, type = "response")
#probability for B: 0.9594669
predict(M3, Aframe, "raw")[,2]
# crab A: 2.071206e-23
predict(M3, Bframe, "raw")[,2]
# crab A: 1
pens = read.csv("~/Github/DSA1101 Slayers/datasets/penguins-dsa1101.csv")
glimpse(pens)
attach(pens)
attach(pens)
plot(bill_dep)
plot(bill_dep, mass)
# Q1
pens = read.csv("~/Github/DSA1101 Slayers/datasets/penguins-dsa1101.csv")
glimpse(pens)
attach(pens)
plot(bill_depth, mass)
wss = numeric(8)
Standardise.X = scale(pens[,])
wss = numeric(8)
for (k in 1:8) {
kout = kmeans(Standardise.X, centers = k)
wss[i] = kout$withinss
}
plot(1:8, wss)
Standardise.X = scale(pens[,])
wss = numeric(8)
for (k in 1:8) {
kout = kmeans(Standardise.X, centers = k)
wss[i] = kout$withinss
wss
}
wss = numeric(8)
for (k in 1:8) {
kout = kmeans(Standardise.X, centers = k)
wss[i] = kout$withinss
print(wss)
}
plot(1:8, wss)
pens = read.csv("~/Github/DSA1101 Slayers/datasets/penguins-dsa1101.csv")
glimpse(pens)
attach(pens)
plot(bill_depth, mass)
#i would choose k =  2 because there are 2 distinct clusters, one in the range of
# mass between 3800 and 6200, and billdepth between 12 and 17.8
# and the other cluster in the range of
# mass between 2500 and 4900, and billdepth between 15.5 and 23
#Q2
Standardise.X = scale(pens[,])
wss = numeric(8)
for (k in 1:8) {
kout = kmeans(Standardise.X, centers = k)
wss[i] = kout$withinss
print(wss)
}
wss = numeric(8)
for (k in 1:8) {
kout = kmeans(Standardise.X, centers = k)
wss[k] = kout$withinss
print(wss)
}
plot(1:8, wss)
kout = kmeans(Standardise.X, centers = 2)
kout$centers
source("~/GitHub/DSA1101 Slayers/Finals Working Materials/A0286548L.R", echo=TRUE)
#perf <- performance(pred , "accuracy")
accuracy = 0.7748 + 0.3226
accuracy
