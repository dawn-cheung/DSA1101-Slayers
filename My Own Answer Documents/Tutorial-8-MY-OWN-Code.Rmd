---
title: "Tutorial 8 & 9 MY OWN Code"
author: "Dawn Cheung"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Naive Bayes

Data set Titanic.csv provides information on the fate of passengers on the fatal maiden voyage of the ocean liner Titanic,. It includes the variables: economic status (class), sex, age and survival. We will train a naive Bayes classifier using this data set, and predict survival


(a) Compute the probabilities P(Y = 1) (survived) and P(Y = 0) (did not survive).
```{r 1a, echo = TRUE}
library(tidyverse)

data = read.csv("~/Github/DSA1101 Slayers/datasets/Titanic.csv")
glimpse(data)

data$Survived = as.factor(data$Survived)
data$Sex = as.factor(data$Sex)
data$Class = as.factor(data$Class)

attach(data)


prop.table(table(Survived))

ProbY1 = prop.table(table(Survived))[[1]] #double bracket to remove the column name
ProbY0 = prop.table(table(Survived))[[2]]

ProbY1
ProbY0


```


(b) Compute the conditional probabilities P(Xi = xi |Y = 1) and P(Xi = xi |Y = 0), where i = 1, 2, 3, 4 for the feature variables X = {class, sex, age}.
```{r 1b, echo = TRUE}
#P(Xi = xi |Y = 1) = P(class = 3rd)
c = table(Class, Survived); c
s = table(Sex, Survived); s
a = table(Age, Survived); a
#completely wrong to use prop.table. we need to get the total divisible to be the entire dataset(?)

c/rowSums(c)
prop.table(c)

#to add all rows: rowSums()




```

(c) Predict survival for an adult female passenger in 2nd class cabin
```{r 1c, echo = TRUE}
#P(Y = survive |Age = Adult, Sex = Female, Class = 2nd)
Yum = ProbY1 * c[[2,2]] * s[[1,2]] * s[[1,2]]

NotYum = ProbY0 * c[[2,1]] * s[[1,1]] * s[[1,1]]

if (Yum > NotYum) {
  print("more likely to survive")
} else {
  print("death is inevitable")
}
```

(d) Compare your prediction in (c) with the one performed by the naiveBayes() function in package `e1071'.
```{r 1d, echo = TRUE}
library("e1071")

M1 <- naiveBayes(Survived ~ Age + Sex + Class, data)#, laplace=0)

newdata = cbind(Age = "Adult", Sex = "Female", Class = "2nd") #maybe use data.frame() instead
predict(M1, newdata = newdata, type = "class")

#prediction is the sameeeee
```



## Naive Bayes + Deision Trees, ROC, AUC

Consider the data set Titanic.csv again.

(a) Fit a decision tree of on all the three feature variables, called M2, which uses minsplit = 1 and information gain.
```{r 2a, echo = TRUE}
library(rpart)
library(rpart.plot)


M2 <- rpart(Survived ~ Age + Sex + Class,
             method = "class",
             data = data,
             control = rpart.control(minsplit = 1),
             parms = list(split = "information"))
summary(M2)

```

(b) Plot the tree M2.
```{r 2b, echo = TRUE}
rpart.plot(M2, type = 4, extra = 2, clip.right.labs = FALSE)
```

(c) Plot the ROC curves and derive the AUC values for the two classifiers (naive Bayes from question 1 and decision tree). Which classifier has larger AUC value?
```{r 2c, echo = TRUE}
library(dplyr)
library(ROCR)

ncol(data)


sur = ifelse(Survived == "Yes", 1,0)

sur = as.factor()

data

# > typeof(data.frame(data[,-ncol(data)]))
# [1] "list"
#HUHHHH

#do NB first
nb_prediction = predict(M1,
    newdata = data.frame(data[,-ncol(data)]),
    type = "raw")

dt_prediction = predict(M2, newdata = data.frame(data[,-ncol(data)]), type = "raw")

nb_predicted_score = nb_prediction[,c("Yes")]
dt_predicted_score = dt_prediction[,c("Yes")]

actual_class = data$Survived == "Yes"

nb_pred = prediction(nb_predicted_score, actual_class)
dt_pred = prediction(dt_predicted_score, actual_class)

nb_perf <- performance(nb_pred , "tpr", "fpr")
dt_perf <- performance(dt_pred , "tpr", "fpr")

auc1 <- performance(pre_nb, "auc")@y.values[[1]]

```


## Logistic Regression (Tutorial 9)

Consider the data set Titanic.csv again.

1) Perform logistic regression using all the feature variables to predict the survival status, called model M2.
```{r 3.1, echo = TRUE}
#hello
```


2) Write down the fitted equation of model M2.
```{r 3.2, echo = TRUE}
#hello
```

3) Interpret the coefficient of the variable `Sex' in M2.
```{r 3.3, echo = TRUE}
#hello
```

4) Interpret the coefficient of the variable `Age' in M2.
```{r 3.4, echo = TRUE}
#hello
```

5) Observe and compare the ROC curves and AUC for the two classifiers: naive Bayes (from Tutorial 8) and logistic regression.
```{r 3.5, echo = TRUE}
#hello
```
